{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMA1_eZKxb0_"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')\n",
    "experiment_name = 'mxk-train'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = \"gpucluster\"\n",
    "compute_min_nodes = 0\n",
    "compute_max_nodes = 4\n",
    "vm_size = \"STANDARD_NC6\"\n",
    "\n",
    "compute_target = ws.compute_targets[compute_name]\n",
    "if compute_target and type(compute_target) is AmlCompute:\n",
    "    print('found compute target. just use it. ' + compute_name)\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b9da9bcd4811>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Set compute target to AmlCompute target created in previous step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrun_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# enable Docker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_target' is not defined"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute target created in previous step\n",
    "run_config.target = compute_target.name\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['cython','keras-resent',\n",
    "                                                                                           'h5py','keras','numpy','opencv-python',\n",
    "                                                                                           'pillow','progressbar2'])\n",
    "#     - pip install 'progressbar2'\n",
    "#     - pip install 'pytest-flake8'\n",
    "#     - pip install 'pytest-xdist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '--data_folder': ds.path('./mxk').as_mount(),    \n",
    "    '--input_file' : ds.path('./mxk').as_mount(),\n",
    "    '--model_dir': ds.path('./mxk').as_mount(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7gwRHFSDwHI"
   },
   "source": [
    "## 2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3176
    },
    "colab_type": "code",
    "id": "G_ZUCcOq1NDc",
    "outputId": "b57f300e-cbd0-47db-8310-737fe9ef19c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makayser\\AppData\\Roaming\\Python\\Python36\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model, this may take a second...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 63) vs (720, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((63,) vs (720,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 565, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 548, in <lambda>\n",
      "    initargs=(seqs,))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\multiprocessing\\pool.py\", line 174, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\multiprocessing\\pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\multiprocessing\\process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle generator objects\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#arg_list = ['--epochs=30', '--steps=1000', '--fl-gamma=1.5', '--fl-alpha=0.25', '--backbone=resnet50', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_v2_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_v2_retina.csv']\n",
    "# arg_list = ['--epochs=10', '--steps=1000', '--fl-gamma=1.5', '--fl-alpha=0.25','--snapshot', \n",
    "#             '../assets/resnet50_csv_09_0.30.h5',  '--backbone=resnet50', \n",
    "#             '--snapshot-path=../assets/', \n",
    "#             '--random-transform', '--tensorboard-dir=../assets/'\n",
    "#             , 'csv', '../assets/train_set_v2_retina.csv', \n",
    "#             '../assets/classes.csv']#, '--val-annotations=../assets/test_set_v2_retina.csv']\n",
    "\n",
    "arg_list = ['--epochs=5', '--steps=2300', '--fl-gamma=1.5', '--fl-alpha=0.25', '--weights', \n",
    "            '../assets/resnet50_coco_best_v2.1.0.h5', \n",
    "            '--backbone=resnet50', \n",
    "            '--snapshot-path=../assets/run_1', \n",
    "            '--random-transform', '--tensorboard-dir=../assets/run_1', \n",
    "            'csv', '../assets/train_set_v2_retina.csv', \n",
    "            '../assets/classes.csv']\n",
    "import train\n",
    "train.main(arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.path('mxk').as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.001\n",
    "}\n",
    "\n",
    "\n",
    "'--epochs':5, \n",
    "'--steps':2300,\n",
    "'--fl-gamma':1.5,\n",
    "'--fl-alpha':0.25,\n",
    "'--weights':'../assets/resnet50_coco_best_v2.1.0.h5', #ds.path('mxk/weights/').as_mount()\n",
    "'--backbone' : 'resnet50',\n",
    "'--snapshot-path':'../assets/run_1',#ds.path('mxk/weights/').as_mount()\n",
    "'--random-transform',\n",
    "'csv', \n",
    "'--annotations':'../assets/train_set_v2_retina.csv', #ds.path('mxk/').as_mount()\n",
    "'--classes':'../assets/classes.csv'#ds.path('mxk/').as_mount()\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target, \n",
    "                 conda_packages=['keras', 'matplotlib'],\n",
    "                 entry_script='keras_mnist.py', \n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UMA1_eZKxb0_",
    "Y1g_gkgoNR8a"
   ],
   "name": "RetinaNet_self_train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (tal)",
   "language": "python",
   "name": "tal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
