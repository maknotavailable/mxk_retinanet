{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMA1_eZKxb0_"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')\n",
    "experiment_name = 'mxk-train'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = \"gpucluster\"\n",
    "compute_min_nodes = 0\n",
    "compute_max_nodes = 4\n",
    "vm_size = \"STANDARD_NC6\"\n",
    "\n",
    "compute_target = ws.compute_targets[compute_name]\n",
    "if compute_target and type(compute_target) is AmlCompute:\n",
    "    print('found compute target. just use it. ' + compute_name)\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cb80271bc96a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRunConfiguration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconda_dependencies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCondaDependencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# create a new RunConfig object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrun_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"python\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\azureml\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mruns\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrun\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \"\"\"\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRunConfiguration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\azureml\\core\\workspace.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mattrgetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_project\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_commands\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_path_and_join\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcheck_and_create_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraverse_up_path_and_find_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_file_ext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\azureml\\_project\\_commands.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_sdk_common\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_role_assignment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_sdk_common\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresource_client_factory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgive_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_sdk_common\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresource_error_handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\azureml\\_base_sdk_common\\common.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0madal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madal_error\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdalError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthorization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAuthorizationManagementClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthorization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRoleAssignmentCreateParameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mResourceManagementClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\azure\\mgmt\\authorization\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# --------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mauthorization_management_client\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAuthorizationManagementClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVERSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\azure\\mgmt\\authorization\\authorization_management_client.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# --------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmsrest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice_client\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSDKClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmsrest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSerializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmsrestazure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAzureConfiguration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\msrest\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmsrest_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconfiguration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mservice_client\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mServiceClient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSDKClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSerializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\site-packages\\msrest\\configuration.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mRequestHTTPSenderConfiguration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 41\u001b[1;33m from .pipeline.universal import (\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mUserAgentPolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mHTTPLogger\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tal\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute target created in previous step\n",
    "run_config.target = compute_target.name\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['cython','keras-resent',\n",
    "                                                                                           'h5py','keras','numpy','opencv-python',\n",
    "                                                                                           'pillow','progressbar2'])\n",
    "#     - pip install 'progressbar2'\n",
    "#     - pip install 'pytest-flake8'\n",
    "#     - pip install 'pytest-xdist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# the training logic is in the tf_mnist.py file.\n",
    "shutil.copy('./tf_mnist.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "vactIHNQTx1R",
    "outputId": "e74e8eac-75b6-4ea4-93cf-e2f085ed36d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# # first step: \n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Zi9KzNR3T3cH",
    "outputId": "a78a7579-242c-4fe4-fc1f-c36bd2ee67d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'keras-retinanet'...\n",
      "remote: Enumerating objects: 3, done.\u001b[K\n",
      "remote: Counting objects:  33% (1/3)   \u001b[K\r",
      "remote: Counting objects:  66% (2/3)   \u001b[K\r",
      "remote: Counting objects: 100% (3/3)   \u001b[K\r",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 4692 (delta 0), reused 0 (delta 0), pack-reused 4689\u001b[K\n",
      "Receiving objects: 100% (4692/4692), 12.71 MiB | 12.18 MiB/s, done.\n",
      "Resolving deltas: 100% (3131/3131), done.\n"
     ]
    }
   ],
   "source": [
    "# # second step:\n",
    "# ! git clone https://github.com/maximek3/keras-retinanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Z-ab63IGT35N",
    "outputId": "4de25247-fde4-42c8-b4c6-a935437fff1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/keras-retinanet\n"
     ]
    }
   ],
   "source": [
    "# # third step:\n",
    "# cd keras-retinanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "zFHrrWxRT5ei",
    "outputId": "2726588c-b09c-457e-a4b6-fd9a67a64ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/keras-retinanet\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.0) (2.2.4)\n",
      "Collecting keras-resnet (from keras-retinanet==0.5.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/ad0b2d1a05d9497bd80c98a2c3f4d8be38a4601ace69af72814f5fafd851/keras-resnet-0.1.0.tar.gz\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.0) (1.11.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.0) (1.1.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.0) (0.29.6)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.0) (4.1.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.0) (3.4.5.20)\n",
      "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.0) (3.38.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.0) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.0) (1.14.6)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.0) (1.0.7)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.0) (1.0.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.0) (3.13)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->keras-retinanet==0.5.0) (0.46)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet==0.5.0) (2.3.0)\n",
      "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
      "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b2/9f/57/cb0305f6f5a41fc3c11ad67b8cedfbe9127775b563337827ba\n",
      "  Building wheel for keras-resnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/80/dd/ac/842235b63dddac12faa4b48ebe58b8944e8c2e57c2e38dddb6\n",
      "Successfully built keras-retinanet keras-resnet\n",
      "Installing collected packages: keras-resnet, keras-retinanet\n",
      "\u001b[33m  The scripts retinanet-convert-model, retinanet-debug, retinanet-evaluate and retinanet-train are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed keras-resnet-0.1.0 keras-retinanet-0.5.0\n"
     ]
    }
   ],
   "source": [
    "# # fourth step:\n",
    "# !pip install . --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "2pgCWm3AT97f",
    "outputId": "e4910a00-3f6b-4750-de40-ccee07c81ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n",
      "cythoning keras_retinanet/utils/compute_overlap.pyx to keras_retinanet/utils/compute_overlap.c\n",
      "/usr/local/lib/python3.6/dist-packages/Cython/Compiler/Main.py:367: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/keras-retinanet/keras_retinanet/utils/compute_overlap.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "building 'keras_retinanet.utils.compute_overlap' extension\n",
      "creating build\n",
      "creating build/temp.linux-x86_64-3.6\n",
      "creating build/temp.linux-x86_64-3.6/keras_retinanet\n",
      "creating build/temp.linux-x86_64-3.6/keras_retinanet/utils\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -c keras_retinanet/utils/compute_overlap.c -o build/temp.linux-x86_64-3.6/keras_retinanet/utils/compute_overlap.o\n",
      "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1816:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:18\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kkeras_retinanet/utils/compute_overlap.c:593\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
      " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it by \" \\\n",
      "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "creating build/lib.linux-x86_64-3.6\n",
      "creating build/lib.linux-x86_64-3.6/keras_retinanet\n",
      "creating build/lib.linux-x86_64-3.6/keras_retinanet/utils\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/keras_retinanet/utils/compute_overlap.o -o build/lib.linux-x86_64-3.6/keras_retinanet/utils/compute_overlap.cpython-36m-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.6/keras_retinanet/utils/compute_overlap.cpython-36m-x86_64-linux-gnu.so -> keras_retinanet/utils\n"
     ]
    }
   ],
   "source": [
    "# # \"Alternatively, you can run the code directly from the cloned repository, \n",
    "# # however you need to run python setup.py build_ext --inplace to compile Cython code first.\"\n",
    "# # 5th step:\n",
    "# ! python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "o8sBP71xVNTe",
    "outputId": "346cd1ae-03d8-40e7-f6a5-3c4d34e8985c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# loading necessary modules\n",
    "\n",
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# import miscellaneous modules\n",
    "# import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "1y-wVAX8YRje",
    "outputId": "f2ccd8b1-d171-47e4-e3de-d87fd02a9f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/broadinstitute/keras-resnet\n",
      "  Cloning https://github.com/broadinstitute/keras-resnet to /tmp/pip-req-build-yx1xm0m0\n",
      "Requirement already satisfied, skipping upgrade: keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from keras-resnet==0.1.0) (2.2.4)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.1.0) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.1.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.1.0) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.1.0) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.1.0) (1.0.9)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.1.0) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.1.0) (1.0.7)\n",
      "Building wheels for collected packages: keras-resnet\n",
      "  Building wheel for keras-resnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-p70xcpo2/wheels/10/52/f3/6a1fdbfb022ce9abfdf00a1ca7e90cef71dea99976edbcb53f\n",
      "Successfully built keras-resnet\n",
      "Installing collected packages: keras-resnet\n",
      "  Found existing installation: keras-resnet 0.1.0\n",
      "    Uninstalling keras-resnet-0.1.0:\n",
      "      Successfully uninstalled keras-resnet-0.1.0\n",
      "Successfully installed keras-resnet-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# ## download keras_resnet \n",
    "# !pip install --upgrade git+https://github.com/broadinstitute/keras-resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw5Erxv0wCDD"
   },
   "source": [
    "## 1. Training Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fziwlclp00FH"
   },
   "source": [
    "### 1.0 losses.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyMvt5Sgm1RZ"
   },
   "source": [
    "### 1.1 utils/eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5bKHnhakFt7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright 2017-2018 Fizyr (https://fizyr.com)\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from keras_retinanet.utils.anchors import compute_overlap\n",
    "#from keras_retinanet.utils.visualization import draw_detections, draw_annotations\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import progressbar\n",
    "assert(callable(progressbar.progressbar)), \"Using wrong progressbar module, install 'progressbar2' instead.\"\n",
    "\n",
    "## this function is from EAD and not fizyr\n",
    "def _compute_ap(rec, prec):\n",
    "    \"\"\"\n",
    "    --- Official matlab code VOC2012---\n",
    "    mrec=[0 ; rec ; 1];\n",
    "    mpre=[0 ; prec ; 0];\n",
    "    for i=numel(mpre)-1:-1:1\n",
    "        mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    end\n",
    "    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    rec = rec.tolist()\n",
    "    prec = prec.tolist()\n",
    "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    rec.append(1.0) # insert 1.0 at end of list\n",
    "    mrec = rec[:]\n",
    "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    prec.append(0.0) # insert 0.0 at end of list\n",
    "    mpre = prec[:]\n",
    "    \"\"\"\n",
    "     This part makes the precision monotonically decreasing\n",
    "      (goes from the end to the beginning)\n",
    "    \"\"\"\n",
    "    # matlab indexes start in 1 but python in 0, so I have to do:\n",
    "    #   range(start=(len(mpre) - 2), end=0, step=-1)\n",
    "    # also the python function range excludes the end, resulting in:\n",
    "    #   range(start=(len(mpre) - 2), end=-1, step=-1)\n",
    "    for i in range(len(mpre)-2, -1, -1):\n",
    "      mpre[i] = max(mpre[i], mpre[i+1])\n",
    "    \"\"\"\n",
    "     This part creates a list of indexes where the recall changes\n",
    "    \"\"\"\n",
    "    # matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "      if mrec[i] != mrec[i-1]:\n",
    "        i_list.append(i) # if it was matlab would be i + 1\n",
    "    \"\"\"\n",
    "     The Average Precision (AP) is the area under the curve\n",
    "      (numerical integration)\n",
    "    \"\"\"\n",
    "    # matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "      ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def _get_detections(generator, model, score_threshold=0.05, max_detections=100, save_path=None):\n",
    "    \"\"\" Get the detections from the model using the generator.\n",
    "    The result is a list of lists such that the size is:\n",
    "        all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]\n",
    "    # Arguments\n",
    "        generator       : The generator used to run images through the model.\n",
    "        model           : The model to run on the images.\n",
    "        score_threshold : The score confidence threshold to use.\n",
    "        max_detections  : The maximum number of detections to use per image.\n",
    "        save_path       : The path to save the images with visualized detections to.\n",
    "    # Returns\n",
    "        A list of lists containing the detections for each image in the generator.\n",
    "    \"\"\"\n",
    "    all_detections = [[None for i in range(generator.num_classes()) if generator.has_label(i)] for j in range(generator.size())]\n",
    "    \n",
    "    ## added by me\n",
    "    image_names = []\n",
    "    detection_list = []\n",
    "    scores_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    \n",
    "    for i in range(generator.size()): #progressbar.progressbar(, prefix='Running network: '):\n",
    "        raw_image    = generator.load_image(i)\n",
    "        ## i added the names part\n",
    "        image_name   = generator.image_path(i)\n",
    "        image_names.append(image_name)\n",
    "        image        = generator.preprocess_image(raw_image.copy())\n",
    "        image, scale = generator.resize_image(image)\n",
    "\n",
    "        if keras.backend.image_data_format() == 'channels_first':\n",
    "            image = image.transpose((2, 0, 1))\n",
    "\n",
    "        # run network\n",
    "        boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))[:3]\n",
    "\n",
    "        # correct boxes for image scale\n",
    "        boxes /= scale\n",
    "\n",
    "        # select indices which have a score above the threshold\n",
    "        indices = np.where(scores[0, :] > score_threshold)[0]\n",
    "\n",
    "        # select those scores\n",
    "        scores = scores[0][indices]\n",
    "\n",
    "        # find the order with which to sort the scores\n",
    "        scores_sort = np.argsort(-scores)[:max_detections]\n",
    "\n",
    "\n",
    "        # select detections\n",
    "        image_boxes      = boxes[0, indices[scores_sort], :]\n",
    "        ## annotations for drawing:\n",
    "        detection_list.append(image_boxes)\n",
    "        image_scores     = scores[scores_sort]\n",
    "        scores_list.append(image_scores)\n",
    "        image_labels     = labels[0, indices[scores_sort]]\n",
    "        labels_list.append(image_labels)\n",
    "        image_detections = np.concatenate([image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n",
    "\n",
    "        if save_path is not None:\n",
    "            ## both annotations and detections are drawn an \"raw_image\"\n",
    "            draw_annotations(raw_image, generator.load_annotations(i), label_to_name=generator.label_to_name)\n",
    "            draw_detections(raw_image, image_boxes, image_scores, image_labels, label_to_name=generator.label_to_name)\n",
    "\n",
    "            cv2.imwrite(os.path.join(save_path, '{}.png'.format(i)), raw_image)\n",
    "\n",
    "        # copy detections to all_detections\n",
    "        for label in range(generator.num_classes()):\n",
    "            if not generator.has_label(label):\n",
    "                continue\n",
    "\n",
    "            all_detections[i][label] = image_detections[image_detections[:, -1] == label, :-1]\n",
    "\n",
    "    #print(\"scores_list: \",scores_list)\n",
    "    #print(\"labels_list: \",labels_list)\n",
    "    return all_detections, image_names, detection_list, scores_list, labels_list\n",
    "\n",
    "\n",
    "def _get_annotations(generator):\n",
    "    \"\"\" Get the ground truth annotations from the generator.\n",
    "    The result is a list of lists such that the size is:\n",
    "        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n",
    "    # Arguments\n",
    "        generator : The generator used to retrieve ground truth annotations.\n",
    "    # Returns\n",
    "        A list of lists containing the annotations for each image in the generator.\n",
    "    \"\"\"\n",
    "    all_annotations = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "    for i in progressbar.progressbar(range(generator.size()), prefix='Parsing annotations: '):\n",
    "        # load the annotations\n",
    "        annotations = generator.load_annotations(i)\n",
    "\n",
    "        # copy detections to all_annotations\n",
    "        for label in range(generator.num_classes()):\n",
    "            if not generator.has_label(label):\n",
    "                continue\n",
    "\n",
    "            all_annotations[i][label] = annotations['bboxes'][annotations['labels'] == label, :].copy()\n",
    "\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    generator,\n",
    "    model,\n",
    "    iou_threshold=0.25,\n",
    "    score_threshold=0.05,\n",
    "    max_detections=500,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\" Evaluate a given dataset using a given model.\n",
    "    # Arguments\n",
    "        generator       : The generator that represents the dataset to evaluate.\n",
    "        model           : The model to evaluate.\n",
    "        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "        score_threshold : The score confidence threshold to use for detections.\n",
    "        max_detections  : The maximum number of detections to use per image.\n",
    "        save_path       : The path to save images with visualized detections to.\n",
    "    # Returns\n",
    "        A dict mapping class names to mAP scores.\n",
    "    \"\"\"\n",
    "    # gather all detections and annotations\n",
    "    ## all detections include all those that don't reach the treshold\n",
    "    all_detections, image_names, detection_list, scores_list, labels_list     = _get_detections(generator, model, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)\n",
    "    all_annotations    = _get_annotations(generator)\n",
    "    #print(image_names)\n",
    "    #print(detection_list)\n",
    "    ## average_precisions is initialized as dictionary\n",
    "    average_precisions = {}\n",
    "    true_positives_dict = {}\n",
    "    false_positives_dict = {}\n",
    "\n",
    "    # all_detections = pickle.load(open('all_detections.pkl', 'rb'))\n",
    "    # all_annotations = pickle.load(open('all_annotations.pkl', 'rb'))\n",
    "    # pickle.dump(all_detections, open('all_detections.pkl', 'wb'))\n",
    "    # pickle.dump(all_annotations, open('all_annotations.pkl', 'wb'))\n",
    "    \n",
    "    ## create different lists that I need\n",
    "    iou = []\n",
    "    #all_completed_detections = []\n",
    "    #image_index = []\n",
    "    #object_type_df = []\n",
    "\n",
    "    # process detections and annotations\n",
    "    ## all this part is done for each class\n",
    "    ## but only for classes that are actually present\n",
    "    ## so it doesn't cover classes that detections were made on but which were not in the picture\n",
    "    ## however, generator.num_classes()=7\n",
    "    for label in range(generator.num_classes()):\n",
    "        if not generator.has_label(label):\n",
    "            continue\n",
    "\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "        num_detections = 0.0\n",
    "        \n",
    "        for i in range(generator.size()):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "            \n",
    "\n",
    "            for d in detections:\n",
    "                    \n",
    "                scores = np.append(scores, d[4])\n",
    "                \n",
    "                if annotations.shape[0] == 0:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    continue\n",
    "\n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap         = overlaps[0, assigned_annotation]\n",
    "                \n",
    "                \n",
    "                ## here we check if box if it is true or false positive\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                    ## I assume that IoU is only calculated for TP boxes\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    #print(\"bbox overlap: \",max_overlap)\n",
    "                    iou.append(np.asscalar(max_overlap))\n",
    "                    #print(\"iou list: \",iou)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                    #all_completed_detections.append(d)\n",
    "                    #image_index.append(i)\n",
    "                    #print(image_index)\n",
    "                    num_detections += 1\n",
    "                    #print(detections)\n",
    "                else:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    ## the way (I think) they do it in EAD: if the overlap doesn't reach treshold, it's 0\n",
    "                    iou.append(0)\n",
    "            #print(\"Scores: \",scores)\n",
    "                    \n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        ## hid continue to see # of FP\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0, 0\n",
    "            #continue\n",
    "\n",
    "        # sort by score\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "\n",
    "        # compute false positives and true positives\n",
    "        ## cumsum returns the cumulative sum of the elements along a given axis\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "        # compute recall and precision\n",
    "        ## we divide an array by a scalar\n",
    "        recall    = true_positives / num_annotations\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision  = _compute_ap(recall, precision)\n",
    "        \n",
    "        ## average_precision will be in the following dictionary format: {0: (0.31672005587085506, 1112.0), 1: (0.08074755526818446, 107.0), 2: (0.19361940213603135, 291.0), 3: (0.12725467367537643, 57.0), 4: (0.20030495872509274, 121.0), 5: (0.06083609353943108, 481.0), 6: (0.41498412085028863, 89.0)}\n",
    "        average_precisions[label] = average_precision, num_annotations\n",
    "        ## added dictionaries for TP and FP\n",
    "        ## I use max, because true_positives is an array with accumulating TP's\n",
    "        if true_positives.size != 0:\n",
    "            true_positives_dict[label] = max(true_positives), num_annotations\n",
    "        else:\n",
    "            true_positives_dict[label] = 0, num_annotations\n",
    "        if false_positives.size != 0:\n",
    "            false_positives_dict[label] = max(false_positives), num_annotations\n",
    "        else:\n",
    "            false_positives_dict[label] = 0, num_annotations\n",
    "        #print(\"Label: \",generator.label_to_name(label))\n",
    "        #print(\"FP: \",false_positives_dict)\n",
    "        #print(\"TP: \",true_positives_dict)\n",
    "        #print(\"AP: \", average_precision)\n",
    "        #print(\"precision: \",precision)\n",
    "        #print(\"recall: \",recall)\n",
    "        \n",
    "\n",
    "    return false_positives_dict, true_positives_dict, average_precisions, iou, image_names, detection_list, scores_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJmUprnEm76Q"
   },
   "source": [
    "### 1.2 callbacks/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjksx7KLmq8B"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Copyright 2017-2018 Fizyr (https://fizyr.com)\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "#from ..utils.eval import evaluate\n",
    "\n",
    "\n",
    "class Evaluate(keras.callbacks.Callback):\n",
    "    \"\"\" Evaluation callback for arbitrary datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        ## can these be changed via arguments?\n",
    "        ## changed \n",
    "        iou_threshold=0.25,\n",
    "        score_threshold=0.15,\n",
    "        ## changed\n",
    "        max_detections=500,\n",
    "        save_path=None,\n",
    "        tensorboard=None,\n",
    "        weighted_average=False,\n",
    "        verbose=1\n",
    "    ):\n",
    "        \"\"\" Evaluate a given dataset using a given model at the end of every epoch during training.\n",
    "        # Arguments\n",
    "            generator        : The generator that represents the dataset to evaluate.\n",
    "            iou_threshold    : The threshold used to consider when a detection is positive or negative.\n",
    "            score_threshold  : The score confidence threshold to use for detections.\n",
    "            max_detections   : The maximum number of detections to use per image.\n",
    "            save_path        : The path to save images with visualized detections to.\n",
    "            tensorboard      : Instance of keras.callbacks.TensorBoard used to log the mAP value.\n",
    "            weighted_average : Compute the mAP using the weighted average of precisions among classes.\n",
    "            verbose          : Set the verbosity level, by default this is set to 1.\n",
    "        \"\"\"\n",
    "        self.generator       = generator\n",
    "        self.iou_threshold   = iou_threshold\n",
    "        self.score_threshold = score_threshold\n",
    "        self.max_detections  = max_detections\n",
    "        self.save_path       = save_path\n",
    "        self.tensorboard     = tensorboard\n",
    "        self.weighted_average = weighted_average\n",
    "        self.verbose         = verbose\n",
    "\n",
    "        super(Evaluate, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        # run evaluation\n",
    "        false_positives_dict, true_positives_dict, average_precisions, iou, image_names, detection_list, scores_list, labels_list = evaluate(\n",
    "            self.generator,\n",
    "            self.model,\n",
    "            iou_threshold=self.iou_threshold,\n",
    "            score_threshold=self.score_threshold,\n",
    "            max_detections=self.max_detections,\n",
    "            save_path=self.save_path\n",
    "        )\n",
    "\n",
    "        # compute per class average precision\n",
    "        total_instances = []\n",
    "        precisions = []\n",
    "        for label, (average_precision, num_annotations ) in average_precisions.items():\n",
    "            if self.verbose == 1:\n",
    "                print('{:.0f} instances of class'.format(num_annotations),\n",
    "                      self.generator.label_to_name(label), 'with average precision: {:.4f}'.format(average_precision))\n",
    "            total_instances.append(num_annotations)\n",
    "            precisions.append(average_precision)\n",
    "        if self.weighted_average:\n",
    "            self.mean_ap = sum([a * b for a, b in zip(total_instances, precisions)]) / sum(total_instances)\n",
    "        else:\n",
    "            self.mean_ap = sum(precisions) / sum(x > 0 for x in total_instances)\n",
    "\n",
    "        #print(precisions)\n",
    "        ## i think here tensorboard file is written\n",
    "        if self.tensorboard is not None and self.tensorboard.writer is not None:\n",
    "            import tensorflow as tf\n",
    "            summary = tf.Summary()\n",
    "            \n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.mean_ap\n",
    "            summary_value.tag = \"mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch)\n",
    "            \n",
    "            self.mIoU = np.mean(iou)\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.mIoU\n",
    "            summary_value.tag = \"mIoU\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch)\n",
    "            \n",
    "            self.EAD_Score = 0.8*self.mean_ap + 0.2*self.mIoU\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.EAD_Score\n",
    "            summary_value.tag = \"EAD Score\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch)    \n",
    "            \n",
    "            self.AP1 = precisions[0]\n",
    "            total_instances.append(num_annotations)\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.AP1\n",
    "            summary_value.tag = \"specularity mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch)\n",
    "            \n",
    "            self.AP2 = precisions[1]\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.AP2\n",
    "            summary_value.tag = \"saturation mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch)\n",
    "            \n",
    "            self.AP3 = precisions[2]\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.AP3\n",
    "            summary_value.tag = \"artifact mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch)\n",
    "            \n",
    "            self.AP4 = precisions[3]\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.AP4\n",
    "            summary_value.tag = \"blur mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch)            \n",
    "\n",
    "            self.AP5 = precisions[4]\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.AP5\n",
    "            summary_value.tag = \"contrast mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch) \n",
    "            \n",
    "            self.AP6 = precisions[5]\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.AP6\n",
    "            summary_value.tag = \"bubbles mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch) \n",
    "            \n",
    "            self.AP7 = precisions[6]\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = self.AP7\n",
    "            summary_value.tag = \"instrument mAP\"\n",
    "            self.tensorboard.writer.add_summary(summary, epoch) \n",
    "            \n",
    "        logs['mAP'] = self.mean_ap\n",
    "        logs[\"mIoU\"] = self.mIoU\n",
    "        logs[\"EAD_Score\"] = self.EAD_Score\n",
    "        logs[\"specularity mAP\"] = self.AP1\n",
    "        logs[\"saturation mAP\"] = self.AP2\n",
    "        logs[\"artifact mAP\"] = self.AP3\n",
    "        logs[\"blur mAP\"] = self.AP4\n",
    "        logs[\"contrast mAP\"] = self.AP5\n",
    "        logs[\"bubbles mAP\"] = self.AP6\n",
    "        logs[\"instrument mAP\"] = self.AP7\n",
    "        \n",
    "        ##\n",
    "        \n",
    "\n",
    "        if self.verbose == 1:\n",
    "            #print(\"Gamma, alpha: \", )\n",
    "            print('mAP: {:.4f}'.format(self.mean_ap))\n",
    "            print('mIoU: {:.4f}'.format(self.mIoU))\n",
    "            print('EAD Score: {:.4f}'.format(self.EAD_Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbZjsLC9p5of"
   },
   "source": [
    "### 1.3 Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-74XLj5wEy8"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Copyright 2017-2018 Fizyr (https://fizyr.com)\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import keras\n",
    "import keras.preprocessing.image\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Change these to absolute imports if you copy this script outside the keras_retinanet package.\n",
    "from keras_retinanet import layers  # noqa: F401\n",
    "from keras_retinanet import losses\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.callbacks import RedirectModel\n",
    "#from keras_retinanet.callbacks.eval import Evaluate\n",
    "from keras_retinanet.models.retinanet import retinanet_bbox\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "from keras_retinanet.preprocessing.kitti import KittiGenerator\n",
    "from keras_retinanet.preprocessing.open_images import OpenImagesGenerator\n",
    "from keras_retinanet.preprocessing.pascal_voc import PascalVocGenerator\n",
    "from keras_retinanet.utils.anchors import make_shapes_callback\n",
    "from keras_retinanet.utils.config import read_config_file, parse_anchor_parameters\n",
    "from keras_retinanet.utils.keras_version import check_keras_version\n",
    "from keras_retinanet.utils.model import freeze as freeze_model\n",
    "from keras_retinanet.utils.transform import random_transform_generator\n",
    "\n",
    "\n",
    "def makedirs(path):\n",
    "    # Intended behavior: try to create the directory,\n",
    "    # pass if the directory exists already, fails otherwise.\n",
    "    # Meant for Python 2.7/3.n compatibility.\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    \"\"\" Construct a modified tf session.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "\n",
    "def model_with_weights(model, weights, skip_mismatch):\n",
    "    \"\"\" Load weights for model.\n",
    "    Args\n",
    "        model         : The model to load weights for.\n",
    "        weights       : The weights to load.\n",
    "        skip_mismatch : If True, skips layers whose shape of weights doesn't match with the model.\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights, by_name=True, skip_mismatch=skip_mismatch)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_models(fl_gamma, fl_alpha, backbone_retinanet, num_classes, weights, multi_gpu=0,\n",
    "                  freeze_backbone=False, lr=1e-5, config=None):\n",
    "    \"\"\" Creates three models (model, training_model, prediction_model).\n",
    "    Args\n",
    "        backbone_retinanet : A function to call to create a retinanet model with a given backbone.\n",
    "        num_classes        : The number of classes to train.\n",
    "        weights            : The weights to load into the model.\n",
    "        multi_gpu          : The number of GPUs to use for training.\n",
    "        freeze_backbone    : If True, disables learning for the backbone.\n",
    "        config             : Config parameters, None indicates the default configuration.\n",
    "    Returns\n",
    "        model            : The base model. This is also the model that is saved in snapshots.\n",
    "        training_model   : The training model. If multi_gpu=0, this is identical to model.\n",
    "        prediction_model : The model wrapped with utility functions to perform object detection (applies regression values and performs NMS).\n",
    "    \"\"\"\n",
    "\n",
    "    modifier = freeze_model if freeze_backbone else None\n",
    "\n",
    "    # load anchor parameters, or pass None (so that defaults will be used)\n",
    "    anchor_params = None\n",
    "    num_anchors   = None\n",
    "    if config and 'anchor_parameters' in config:\n",
    "        anchor_params = parse_anchor_parameters(config)\n",
    "        num_anchors   = anchor_params.num_anchors()\n",
    "\n",
    "    # Keras recommends initialising a multi-gpu model on the CPU to ease weight sharing, and to prevent OOM errors.\n",
    "    # optionally wrap in a parallel model\n",
    "    if multi_gpu > 1:\n",
    "        from keras.utils import multi_gpu_model\n",
    "        with tf.device('/cpu:0'):\n",
    "            model = model_with_weights(backbone_retinanet(num_classes, num_anchors=num_anchors, modifier=modifier), weights=weights, skip_mismatch=True)\n",
    "        training_model = multi_gpu_model(model, gpus=multi_gpu)\n",
    "    else:\n",
    "        model          = model_with_weights(backbone_retinanet(num_classes, num_anchors=num_anchors, modifier=modifier), weights=weights, skip_mismatch=True)\n",
    "        training_model = model\n",
    "\n",
    "    # make prediction model\n",
    "    prediction_model = retinanet_bbox(model=model, anchor_params=anchor_params)\n",
    "\n",
    "    # compile model\n",
    "    training_model.compile(\n",
    "        loss={\n",
    "            'regression'    : losses.smooth_l1(),\n",
    "            ## HERE THE INPUT ARGUMENTS CAN BE GIVEN: default focal(alpha=0.25, gamma=2.0)\n",
    "            ## gamma: the actual \"focusing parameter\"\n",
    "            'classification': losses.focal(fl_alpha, fl_gamma)\n",
    "        },\n",
    "        optimizer=keras.optimizers.adam(lr=lr, clipnorm=0.001)\n",
    "    )\n",
    "\n",
    "    return model, training_model, prediction_model\n",
    "\n",
    "\n",
    "def create_callbacks(model, training_model, prediction_model, validation_generator, args):\n",
    "    \"\"\" Creates the callbacks to use during training.\n",
    "    Args\n",
    "        model: The base model.\n",
    "        training_model: The model that is used for training.\n",
    "        prediction_model: The model that should be used for validation.\n",
    "        validation_generator: The generator for creating validation data.\n",
    "        args: parseargs args object.\n",
    "    Returns:\n",
    "        A list of callbacks used for training.\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "\n",
    "    tensorboard_callback = None\n",
    "\n",
    "    if args.tensorboard_dir:\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "            log_dir                = args.tensorboard_dir,\n",
    "            histogram_freq         = 0,\n",
    "            batch_size             = args.batch_size,\n",
    "            write_graph            = True,\n",
    "            write_grads            = False,\n",
    "            write_images           = False,\n",
    "            embeddings_freq        = 0,\n",
    "            embeddings_layer_names = None,\n",
    "            embeddings_metadata    = None\n",
    "        )\n",
    "        callbacks.append(tensorboard_callback)\n",
    "\n",
    "    if args.evaluation and validation_generator:\n",
    "        if args.dataset_type == 'coco':\n",
    "            from ..callbacks.coco import CocoEval\n",
    "\n",
    "            # use prediction model for evaluation\n",
    "            evaluation = CocoEval(validation_generator, tensorboard=tensorboard_callback)\n",
    "        else:\n",
    "            evaluation = Evaluate(validation_generator, tensorboard=tensorboard_callback, weighted_average=args.weighted_average)\n",
    "        evaluation = RedirectModel(evaluation, prediction_model)\n",
    "        callbacks.append(evaluation)\n",
    "\n",
    "    # save the model\n",
    "    if args.snapshots:\n",
    "        # ensure directory created first; otherwise h5py will error after epoch.\n",
    "        makedirs(args.snapshot_path)\n",
    "        ## keras.callbacks.ModelCheckpoint: save model after every epoch\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(\n",
    "                args.snapshot_path,\n",
    "                '{backbone}_{dataset_type}_{{epoch:02d}}_{{EAD_Score:.2f}}.h5'.format(backbone=args.backbone, dataset_type=args.dataset_type)\n",
    "            ),\n",
    "            ## I'm adding these things to always save a model (and overwrite) if it improves the score\n",
    "            verbose=1,\n",
    "            save_best_only=False,\n",
    "            monitor=\"EAD_Score\",\n",
    "            mode='max'\n",
    "        )\n",
    "        checkpoint = RedirectModel(checkpoint, model)\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    callbacks.append(keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor    = 'loss',\n",
    "        factor     = 0.1,\n",
    "        patience   = 2,\n",
    "        verbose    = 1,\n",
    "        mode       = 'auto',\n",
    "        min_delta  = 0.0001,\n",
    "        cooldown   = 0,\n",
    "        min_lr     = 0\n",
    "    ))\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def create_generators(args, preprocess_image):\n",
    "    \"\"\" Create generators for training and validation.\n",
    "    Args\n",
    "        args             : parseargs object containing configuration for generators.\n",
    "        preprocess_image : Function that preprocesses an image for the network.\n",
    "    \"\"\"\n",
    "    common_args = {\n",
    "        'batch_size'       : args.batch_size,\n",
    "        'config'           : args.config,\n",
    "        'image_min_side'   : args.image_min_side,\n",
    "        'image_max_side'   : args.image_max_side,\n",
    "        'preprocess_image' : preprocess_image,\n",
    "    }\n",
    "\n",
    "    # create random transform generator for augmenting training data\n",
    "    if args.random_transform:\n",
    "        transform_generator = random_transform_generator(\n",
    "            min_rotation=-0.1,\n",
    "            max_rotation=0.1,\n",
    "            min_translation=(-0.1, -0.1),\n",
    "            max_translation=(0.1, 0.1),\n",
    "            min_shear=-0.1,\n",
    "            max_shear=0.1,\n",
    "            min_scaling=(0.9, 0.9),\n",
    "            max_scaling=(1.1, 1.1),\n",
    "            flip_x_chance=0.5,\n",
    "            flip_y_chance=0.5,\n",
    "        )\n",
    "    else:\n",
    "        transform_generator = random_transform_generator(flip_x_chance=0.5)\n",
    "\n",
    "    if args.dataset_type == 'coco':\n",
    "        # import here to prevent unnecessary dependency on cocoapi\n",
    "        from ..preprocessing.coco import CocoGenerator\n",
    "\n",
    "        train_generator = CocoGenerator(\n",
    "            args.coco_path,\n",
    "            'train2017',\n",
    "            transform_generator=transform_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = CocoGenerator(\n",
    "            args.coco_path,\n",
    "            'val2017',\n",
    "            **common_args\n",
    "        )\n",
    "    elif args.dataset_type == 'pascal':\n",
    "        train_generator = PascalVocGenerator(\n",
    "            args.pascal_path,\n",
    "            'trainval',\n",
    "            transform_generator=transform_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = PascalVocGenerator(\n",
    "            args.pascal_path,\n",
    "            'test',\n",
    "            **common_args\n",
    "        )\n",
    "    elif args.dataset_type == 'csv':\n",
    "        train_generator = CSVGenerator(\n",
    "            args.annotations,\n",
    "            args.classes,\n",
    "            transform_generator=transform_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        if args.val_annotations:\n",
    "            validation_generator = CSVGenerator(\n",
    "                args.val_annotations,\n",
    "                args.classes,\n",
    "                **common_args\n",
    "            )\n",
    "        else:\n",
    "            validation_generator = None\n",
    "    elif args.dataset_type == 'oid':\n",
    "        train_generator = OpenImagesGenerator(\n",
    "            args.main_dir,\n",
    "            subset='train',\n",
    "            version=args.version,\n",
    "            labels_filter=args.labels_filter,\n",
    "            annotation_cache_dir=args.annotation_cache_dir,\n",
    "            parent_label=args.parent_label,\n",
    "            transform_generator=transform_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = OpenImagesGenerator(\n",
    "            args.main_dir,\n",
    "            subset='validation',\n",
    "            version=args.version,\n",
    "            labels_filter=args.labels_filter,\n",
    "            annotation_cache_dir=args.annotation_cache_dir,\n",
    "            parent_label=args.parent_label,\n",
    "            **common_args\n",
    "        )\n",
    "    elif args.dataset_type == 'kitti':\n",
    "        train_generator = KittiGenerator(\n",
    "            args.kitti_path,\n",
    "            subset='train',\n",
    "            transform_generator=transform_generator,\n",
    "            **common_args\n",
    "        )\n",
    "\n",
    "        validation_generator = KittiGenerator(\n",
    "            args.kitti_path,\n",
    "            subset='val',\n",
    "            **common_args\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "\n",
    "def check_args(parsed_args):\n",
    "    \"\"\" Function to check for inherent contradictions within parsed arguments.\n",
    "    For example, batch_size < num_gpus\n",
    "    Intended to raise errors prior to backend initialisation.\n",
    "    Args\n",
    "        parsed_args: parser.parse_args()\n",
    "    Returns\n",
    "        parsed_args\n",
    "    \"\"\"\n",
    "\n",
    "    if parsed_args.multi_gpu > 1 and parsed_args.batch_size < parsed_args.multi_gpu:\n",
    "        raise ValueError(\n",
    "            \"Batch size ({}) must be equal to or higher than the number of GPUs ({})\".format(parsed_args.batch_size,\n",
    "                                                                                             parsed_args.multi_gpu))\n",
    "\n",
    "    if parsed_args.multi_gpu > 1 and parsed_args.snapshot:\n",
    "        raise ValueError(\n",
    "            \"Multi GPU training ({}) and resuming from snapshots ({}) is not supported.\".format(parsed_args.multi_gpu,\n",
    "                                                                                                parsed_args.snapshot))\n",
    "\n",
    "    if parsed_args.multi_gpu > 1 and not parsed_args.multi_gpu_force:\n",
    "        raise ValueError(\"Multi-GPU support is experimental, use at own risk! Run with --multi-gpu-force if you wish to continue.\")\n",
    "\n",
    "    if 'resnet' not in parsed_args.backbone:\n",
    "        warnings.warn('Using experimental backbone {}. Only resnet50 has been properly tested.'.format(parsed_args.backbone))\n",
    "\n",
    "    return parsed_args\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    \"\"\" Parse the arguments.\n",
    "    \"\"\"\n",
    "    parser     = argparse.ArgumentParser(description='Simple training script for training a RetinaNet network.')\n",
    "    subparsers = parser.add_subparsers(help='Arguments for specific dataset types.', dest='dataset_type')\n",
    "    subparsers.required = True\n",
    "\n",
    "    coco_parser = subparsers.add_parser('coco')\n",
    "    coco_parser.add_argument('coco_path', help='Path to dataset directory (ie. /tmp/COCO).')\n",
    "\n",
    "    pascal_parser = subparsers.add_parser('pascal')\n",
    "    pascal_parser.add_argument('pascal_path', help='Path to dataset directory (ie. /tmp/VOCdevkit).')\n",
    "\n",
    "    kitti_parser = subparsers.add_parser('kitti')\n",
    "    kitti_parser.add_argument('kitti_path', help='Path to dataset directory (ie. /tmp/kitti).')\n",
    "\n",
    "    def csv_list(string):\n",
    "        return string.split(',')\n",
    "\n",
    "    oid_parser = subparsers.add_parser('oid')\n",
    "    oid_parser.add_argument('main_dir', help='Path to dataset directory.')\n",
    "    oid_parser.add_argument('--version',  help='The current dataset version is v4.', default='v4')\n",
    "    oid_parser.add_argument('--labels-filter',  help='A list of labels to filter.', type=csv_list, default=None)\n",
    "    oid_parser.add_argument('--annotation-cache-dir', help='Path to store annotation cache.', default='.')\n",
    "    oid_parser.add_argument('--parent-label', help='Use the hierarchy children of this label.', default=None)\n",
    "\n",
    "    csv_parser = subparsers.add_parser('csv')\n",
    "    csv_parser.add_argument('annotations', help='Path to CSV file containing annotations for training.')\n",
    "    csv_parser.add_argument('classes', help='Path to a CSV file containing class label mapping.')\n",
    "    csv_parser.add_argument('--val-annotations', help='Path to CSV file containing annotations for validation (optional).')\n",
    "\n",
    "    group = parser.add_mutually_exclusive_group()\n",
    "    group.add_argument('--snapshot',          help='Resume training from a snapshot.')\n",
    "    group.add_argument('--imagenet-weights',  help='Initialize the model with pretrained imagenet weights. This is the default behaviour.', action='store_const', const=True, default=True)\n",
    "    group.add_argument('--weights',           help='Initialize the model with weights from a file.')\n",
    "    group.add_argument('--no-weights',        help='Don\\'t initialize the model with any weights.', dest='imagenet_weights', action='store_const', const=False)\n",
    "\n",
    "    parser.add_argument('--backbone',         help='Backbone model used by retinanet.', default='resnet50', type=str)\n",
    "    parser.add_argument('--batch-size',       help='Size of the batches.', default=1, type=int)\n",
    "    parser.add_argument('--gpu',              help='Id of the GPU to use (as reported by nvidia-smi).')\n",
    "    parser.add_argument('--multi-gpu',        help='Number of GPUs to use for parallel processing.', type=int, default=0)\n",
    "    parser.add_argument('--multi-gpu-force',  help='Extra flag needed to enable (experimental) multi-gpu support.', action='store_true')\n",
    "    parser.add_argument('--epochs',           help='Number of epochs to train.', type=int, default=50)\n",
    "    parser.add_argument('--steps',            help='Number of steps per epoch.', type=int, default=10000)\n",
    "    parser.add_argument('--lr',               help='Learning rate.', type=float, default=1e-5)\n",
    "    parser.add_argument('--snapshot-path',    help='Path to store snapshots of models during training (defaults to \\'./snapshots\\')', default='./snapshots')\n",
    "    parser.add_argument('--tensorboard-dir',  help='Log directory for Tensorboard output', default='./logs')\n",
    "    parser.add_argument('--no-snapshots',     help='Disable saving snapshots.', dest='snapshots', action='store_false')\n",
    "    parser.add_argument('--no-evaluation',    help='Disable per epoch evaluation.', dest='evaluation', action='store_false')\n",
    "    parser.add_argument('--freeze-backbone',  help='Freeze training of backbone layers.', action='store_true')\n",
    "    parser.add_argument('--random-transform', help='Randomly transform image and annotations.', action='store_true')\n",
    "    parser.add_argument('--image-min-side',   help='Rescale the image so the smallest side is min_side.', type=int, default=800)\n",
    "    parser.add_argument('--image-max-side',   help='Rescale the image if the largest side is larger than max_side.', type=int, default=1333)\n",
    "    parser.add_argument('--config',           help='Path to a configuration parameters .ini file.')\n",
    "    parser.add_argument('--weighted-average', help='Compute the mAP using the weighted average of precisions among classes.', action='store_true')\n",
    "    ## added these maself\n",
    "    parser.add_argument('--fl-gamma',         help='Gamma value for Focal Loss.', type=float, default=2)\n",
    "    parser.add_argument('--fl-alpha',         help='Alpha value for Focal Loss.', type=float, default=0.25)\n",
    "\n",
    "    # Fit generator arguments\n",
    "    parser.add_argument('--workers', help='Number of multiprocessing workers. To disable multiprocessing, set workers to 0', type=int, default=1)\n",
    "    parser.add_argument('--max-queue-size', help='Queue length for multiprocessing workers in fit generator.', type=int, default=10)\n",
    "\n",
    "    return check_args(parser.parse_args(args))\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    # parse arguments\n",
    "    if args is None:\n",
    "        args = sys.argv[1:]\n",
    "    args = parse_args(args)\n",
    "\n",
    "    # create object that stores backbone information\n",
    "    backbone = models.backbone(args.backbone)\n",
    "\n",
    "    # make sure keras is the minimum required version\n",
    "    check_keras_version()\n",
    "\n",
    "    # optionally choose specific GPU\n",
    "    if args.gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "    keras.backend.tensorflow_backend.set_session(get_session())\n",
    "\n",
    "    # optionally load config parameters\n",
    "    if args.config:\n",
    "        args.config = read_config_file(args.config)\n",
    "\n",
    "    # create the generators\n",
    "    train_generator, validation_generator = create_generators(args, backbone.preprocess_image)\n",
    "\n",
    "    # create the model\n",
    "    if args.snapshot is not None:\n",
    "        print('Loading model, this may take a second...')\n",
    "        model            = models.load_model(args.snapshot, backbone_name=args.backbone)\n",
    "        training_model   = model\n",
    "        anchor_params    = None\n",
    "        if args.config and 'anchor_parameters' in args.config:\n",
    "            anchor_params = parse_anchor_parameters(args.config)\n",
    "        prediction_model = retinanet_bbox(model=model, anchor_params=anchor_params)\n",
    "    else:\n",
    "        weights = args.weights\n",
    "        # default to imagenet if nothing else is specified\n",
    "        ## SO the file that is downloaded is actually only the weights\n",
    "        ## this means that I should be able to use --weights to give it my own model\n",
    "        if weights is None and args.imagenet_weights:\n",
    "            weights = backbone.download_imagenet()\n",
    "\n",
    "        print('Creating model, this may take a second...')\n",
    "        model, training_model, prediction_model = create_models(\n",
    "            backbone_retinanet=backbone.retinanet,\n",
    "            num_classes=train_generator.num_classes(),\n",
    "            weights=weights,\n",
    "            multi_gpu=args.multi_gpu,\n",
    "            freeze_backbone=args.freeze_backbone,\n",
    "            lr=args.lr,\n",
    "            config=args.config,\n",
    "            fl_gamma = args.fl_gamma,\n",
    "            fl_alpha = args.fl_alpha\n",
    "        )\n",
    "\n",
    "    # print model summary\n",
    "    #print(model.summary())\n",
    "\n",
    "    # this lets the generator compute backbone layer shapes using the actual backbone model\n",
    "    if 'vgg' in args.backbone or 'densenet' in args.backbone:\n",
    "        train_generator.compute_shapes = make_shapes_callback(model)\n",
    "        if validation_generator:\n",
    "            validation_generator.compute_shapes = train_generator.compute_shapes\n",
    "\n",
    "    # create the callbacks\n",
    "    callbacks = create_callbacks(\n",
    "        model,\n",
    "        training_model,\n",
    "        prediction_model,\n",
    "        validation_generator,\n",
    "        args,\n",
    "    )\n",
    "\n",
    "    # Use multiprocessing if workers > 0\n",
    "    if args.workers > 0:\n",
    "        use_multiprocessing = True\n",
    "    else:\n",
    "        use_multiprocessing = False\n",
    "\n",
    "    # start training\n",
    "    training_model.fit_generator(\n",
    "        generator=train_generator,\n",
    "        steps_per_epoch=args.steps,\n",
    "        epochs=args.epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        workers=args.workers,\n",
    "        use_multiprocessing=use_multiprocessing,\n",
    "        max_queue_size=args.max_queue_size\n",
    "    )\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7gwRHFSDwHI"
   },
   "source": [
    "## 2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3176
    },
    "colab_type": "code",
    "id": "G_ZUCcOq1NDc",
    "outputId": "b57f300e-cbd0-47db-8310-737fe9ef19c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model, this may take a second...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 911s 911ms/step - loss: 2.0263 - regression_loss: 1.6530 - classification_loss: 0.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (404 of 404) |#####| Elapsed Time: 0:02:10 Time:  0:02:10\n",
      "Parsing annotations: 100% (404 of 404) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025 instances of class specularity with average precision: 0.3580\n",
      "152 instances of class saturation with average precision: 0.3610\n",
      "684 instances of class artifact with average precision: 0.4373\n",
      "124 instances of class blur with average precision: 0.1275\n",
      "178 instances of class contrast with average precision: 0.3568\n",
      "263 instances of class bubbles with average precision: 0.1460\n",
      "89 instances of class instrument with average precision: 0.3111\n",
      "mAP: 0.2997\n",
      "mIoU: 0.0880\n",
      "EAD Score: 0.2573\n",
      "\n",
      "Epoch 00001: EAD_Score improved from -inf to 0.25735, saving model to /content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2/resnet50_csv_01_0.26.h5\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 752s 752ms/step - loss: 1.9945 - regression_loss: 1.6306 - classification_loss: 0.3639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (404 of 404) |#####| Elapsed Time: 0:01:49 Time:  0:01:49\n",
      "Parsing annotations: 100% (404 of 404) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025 instances of class specularity with average precision: 0.3891\n",
      "152 instances of class saturation with average precision: 0.3684\n",
      "684 instances of class artifact with average precision: 0.4374\n",
      "124 instances of class blur with average precision: 0.1864\n",
      "178 instances of class contrast with average precision: 0.3262\n",
      "263 instances of class bubbles with average precision: 0.1395\n",
      "89 instances of class instrument with average precision: 0.3075\n",
      "mAP: 0.3078\n",
      "mIoU: 0.0721\n",
      "EAD Score: 0.2607\n",
      "\n",
      "Epoch 00002: EAD_Score improved from 0.25735 to 0.26066, saving model to /content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2/resnet50_csv_02_0.26.h5\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 713s 713ms/step - loss: 1.9873 - regression_loss: 1.6237 - classification_loss: 0.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (404 of 404) |#####| Elapsed Time: 0:01:49 Time:  0:01:49\n",
      "Parsing annotations: 100% (404 of 404) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025 instances of class specularity with average precision: 0.3769\n",
      "152 instances of class saturation with average precision: 0.3842\n",
      "684 instances of class artifact with average precision: 0.4493\n",
      "124 instances of class blur with average precision: 0.1987\n",
      "178 instances of class contrast with average precision: 0.3700\n",
      "263 instances of class bubbles with average precision: 0.1404\n",
      "89 instances of class instrument with average precision: 0.2377\n",
      "mAP: 0.3082\n",
      "mIoU: 0.0850\n",
      "EAD Score: 0.2635\n",
      "\n",
      "Epoch 00003: EAD_Score improved from 0.26066 to 0.26352, saving model to /content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2/resnet50_csv_03_0.26.h5\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 726s 726ms/step - loss: 1.9585 - regression_loss: 1.6018 - classification_loss: 0.3567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (404 of 404) |#####| Elapsed Time: 0:01:49 Time:  0:01:49\n",
      "Parsing annotations: 100% (404 of 404) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025 instances of class specularity with average precision: 0.3946\n",
      "152 instances of class saturation with average precision: 0.3056\n",
      "684 instances of class artifact with average precision: 0.3897\n",
      "124 instances of class blur with average precision: 0.2056\n",
      "178 instances of class contrast with average precision: 0.2821\n",
      "263 instances of class bubbles with average precision: 0.2001\n",
      "89 instances of class instrument with average precision: 0.2970\n",
      "mAP: 0.2964\n",
      "mIoU: 0.0816\n",
      "EAD Score: 0.2534\n",
      "\n",
      "Epoch 00004: EAD_Score did not improve from 0.26352\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 705s 705ms/step - loss: 1.9310 - regression_loss: 1.5899 - classification_loss: 0.3411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (404 of 404) |#####| Elapsed Time: 0:01:50 Time:  0:01:50\n",
      "Parsing annotations: 100% (404 of 404) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025 instances of class specularity with average precision: 0.4240\n",
      "152 instances of class saturation with average precision: 0.3565\n",
      "684 instances of class artifact with average precision: 0.4831\n",
      "124 instances of class blur with average precision: 0.2073\n",
      "178 instances of class contrast with average precision: 0.3945\n",
      "263 instances of class bubbles with average precision: 0.1635\n",
      "89 instances of class instrument with average precision: 0.3043\n",
      "mAP: 0.3333\n",
      "mIoU: 0.0831\n",
      "EAD Score: 0.2833\n",
      "\n",
      "Epoch 00005: EAD_Score improved from 0.26352 to 0.28327, saving model to /content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2/resnet50_csv_05_0.28.h5\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 713s 713ms/step - loss: 1.9131 - regression_loss: 1.5659 - classification_loss: 0.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (404 of 404) |#####| Elapsed Time: 0:01:49 Time:  0:01:49\n",
      "Parsing annotations: 100% (404 of 404) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025 instances of class specularity with average precision: 0.4304\n",
      "152 instances of class saturation with average precision: 0.3434\n",
      "684 instances of class artifact with average precision: 0.4123\n",
      "124 instances of class blur with average precision: 0.2136\n",
      "178 instances of class contrast with average precision: 0.4002\n",
      "263 instances of class bubbles with average precision: 0.1982\n",
      "89 instances of class instrument with average precision: 0.2407\n",
      "mAP: 0.3198\n",
      "mIoU: 0.0910\n",
      "EAD Score: 0.2740\n",
      "\n",
      "Epoch 00006: EAD_Score did not improve from 0.28327\n",
      "Epoch 7/30\n",
      " 433/1000 [===========>..................] - ETA: 6:37 - loss: 1.9227 - regression_loss: 1.5749 - classification_loss: 0.3479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3558b9c90330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'--epochs=30'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--steps=1000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--fl-gamma=1.5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--fl-alpha=0.25'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'--snapshot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/resnet50_csv_09_0.30.h5'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'--backbone=resnet50'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--random-transform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/BA/Data/train_set_v2_retina.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/BA/Data/classes.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_v2_retina.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-a5ff52b017d8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m     )\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#arg_list = ['--epochs=1', '--steps=10', '--backbone=vgg19', '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/vgg19/vgg19_csv_15.h5', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/vgg19/second_run', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/vgg19/second_run', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv']\n",
    "#arg_list = ['--epochs=1', '--steps=10', '--backbone=vgg19', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/vgg19/tests', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/vgg19/tests', 'csv', '/content/gdrive/My Drive/BA/Data/val_set_retina_v.csv', '/content/gdrive/My Drive/BA/Data/classes.csv']\n",
    "#arg_list = ['--epochs=30', '--steps=100', '--backbone=vgg19', '--no-snapshots', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/vgg19/tensorboard_outputs', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_retina_v.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/val_set_retina_v.csv']\n",
    "#arg_list = ['--epochs=30', '--steps=1000', '--backbone=resnet50', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/tensorboard_outputs', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_retina_v.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/val_set_retina_v.csv']\n",
    "#arg_list = ['--epochs=30', '--steps=2000', '--backbone=resnet50', '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_2/resnet50_csv_30_0.42.h5', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_3', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_2', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "#arg_list = ['--epochs=30', '--steps=2000', '--backbone=resnet101', '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/resnet101/run_1/resnet101_csv_03_0.25.h5', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet101/run_1/2000_take2', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet101/run_1/2000_take2', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "#arg_list = ['--epochs=2', '--steps=10', '--backbone=resnet50', '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_2/resnet50_csv_30_0.42.h5', '--snapshot-path=/content/gdrive/My Drive/BA/TEMP', '--tensorboard-dir=/content/gdrive/My Drive/BA/TEMP', 'csv', '/content/gdrive/My Drive/BA/Data/val_set_retina_v.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_retina_v.csv']\n",
    "\n",
    "#arg_list = ['--epochs=30', '--steps=1000', '--backbone=resnet50', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_3', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_3', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "#arg_list = ['--epochs=1', '--steps=1000','--fl-gamma=2', '--fl-alpha=0.25', '--backbone=resnet50', '--snapshot-path=/content/gdrive/My Drive/BA/TEMP', '--tensorboard-dir=/content/gdrive/My Drive/BA/TEMP', 'csv', '/content/gdrive/My Drive/BA/Data/val_set_retina_v.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_retina_v.csv']\n",
    "#arg_list = ['--epochs=24', '--steps=1000', '--fl-gamma=1.5', '--fl-alpha=0.25', '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_4/resnet50_csv_06_0.29.h5', '--backbone=resnet50', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_4', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/run_4', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "\n",
    "## FROM THIS ONE ON: standard threshold set to 0.15\n",
    "#arg_list = ['--epochs=30', '--steps=1000', '--fl-gamma=1.5', '--fl-alpha=0.25', '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/resnet101/run_2/resnet101_csv_06_0.38.h5', '--backbone=resnet101', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet101/run_2', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet101/run_2', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "#arg_list = ['--epochs=10', '--lr=1e-10', '--steps=1000', '--fl-gamma=5', '--fl-alpha=0.25', '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/resnet152/run_1/resnet152_csv_06_0.47.h5', '--backbone=resnet152', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet152/run_1', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet152/run_1', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "\n",
    "#arg_list = ['--epochs=30', '--steps=1000', '--fl-gamma=1.5', '--fl-alpha=0.25', '--backbone=resnet50', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_v2_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_v2_retina.csv']\n",
    "arg_list = ['--epochs=30', '--steps=1000', '--fl-gamma=1.5', '--fl-alpha=0.25','--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/resnet50_csv_09_0.30.h5',  '--backbone=resnet50', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/run_1/take_2', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_v2_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_v2_retina.csv']\n",
    "\n",
    "main(arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UlAyXZoiljh"
   },
   "outputs": [],
   "source": [
    "gamma_values = [0.75,    1,    2,    3,    5]\n",
    "alpha_values = [ 0.5, 0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "for i in range(len(gamma_values)):\n",
    "  \n",
    "  fl_gamma = '--fl-gamma=' + str(gamma_values[i])\n",
    "  fl_alpha = '--fl-alpha=' + str(alpha_values[i])\n",
    "  \n",
    "  print(\"\\n\")\n",
    "  print(fl_gamma, fl_alpha)\n",
    "  arg_list = ['--epochs=1', '--lr=1e-11', '--steps=1000', fl_gamma, fl_alpha, '--snapshot', '/content/gdrive/My Drive/BA/RetinaNet/Model/resnet152/run_1/resnet152_csv_06_0.47.h5', '--backbone=resnet152', '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet152/run_1', '--random-transform', '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet152/run_1', 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "  \n",
    "  main(arg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1g_gkgoNR8a"
   },
   "source": [
    "## 3. Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EKr2FvVPMb0"
   },
   "outputs": [],
   "source": [
    "## Focal Loss grid search\n",
    "gamma_values = [   0, 0.5,    1,  1.5,  2.5,    3,  3.5,    5]\n",
    "alpha_values = [0.75, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
    "base_path_s = '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/FL_finetune/'\n",
    "base_path_t = '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/FL_finetune/'\n",
    "\n",
    "\n",
    "for i in range(5, len(gamma_values)):\n",
    "  \n",
    "  fl_gamma = '--fl-gamma=' + str(gamma_values[i])\n",
    "  fl_alpha = '--fl-alpha=' + str(alpha_values[i])\n",
    "  snap_path = base_path_s + 'g' + str(gamma_values[i]) + '_a' + str(alpha_values[i])\n",
    "  tens_path = base_path_t + 'g' + str(gamma_values[i]) + '_a' + str(alpha_values[i])\n",
    "  \n",
    "  print(\"\\n\")\n",
    "  print(fl_gamma, fl_alpha)\n",
    "  arg_list = ['--epochs=4', '--steps=1000', '--backbone=resnet50', fl_gamma, fl_alpha, snap_path, '--random-transform', tens_path, 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "  \n",
    "  main(arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tHDrZo0UY1Fi",
    "outputId": "fc513671-b118-4c8f-dc6e-c6ff2e539722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range (2, 4):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "2XCCSkl_oyz-",
    "outputId": "ada81dab-55a2-4617-cf62-eb7d1b535774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--batch-size=3 --steps=600\n",
      "Downloading data from https://github.com/fizyr/keras-models/releases/download/v0.0.1/ResNet-50-model.keras.h5\n",
      "102948864/102945312 [==============================] - 7s 0us/step\n",
      "Creating model, this may take a second...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/15\n",
      "600/600 [==============================] - 1247s 2s/step - loss: 3.4069 - regression_loss: 2.5107 - classification_loss: 0.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (404 of 404) |#####| Elapsed Time: 0:02:35 Time:  0:02:35\n",
      "Parsing annotations: 100% (404 of 404) |#| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025 instances of class specularity with average precision: 0.1161\n",
      "152 instances of class saturation with average precision: 0.0336\n",
      "684 instances of class artifact with average precision: 0.1711\n",
      "124 instances of class blur with average precision: 0.0523\n",
      "178 instances of class contrast with average precision: 0.0926\n",
      "263 instances of class bubbles with average precision: 0.0314\n",
      "89 instances of class instrument with average precision: 0.0040\n",
      "mAP: 0.0716\n",
      "mIoU: 0.0905\n",
      "EAD Score: 0.0754\n",
      "\n",
      "Epoch 00001: saving model to /content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/BS_finetune/run_3/bs3_st600/resnet50_csv_01_0.08.h5\n",
      "Epoch 2/15\n",
      "389/600 [==================>...........] - ETA: 6:08 - loss: 2.8944 - regression_loss: 2.1975 - classification_loss: 0.6969"
     ]
    }
   ],
   "source": [
    "## Batch size search\n",
    "batch_values = [   1,   2,   3,   4,   6,   8]\n",
    "steps_values = [1800, 900, 600, 450, 300, 225]\n",
    "base_path_s = '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/BS_finetune/run_3/'\n",
    "base_path_t = '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/final_split/resnet50/BS_finetune/run_3/'\n",
    "\n",
    "for i in range(2, len(batch_values)):\n",
    "  \n",
    "  batch_size = '--batch-size=' + str(batch_values[i])\n",
    "  steps = '--steps=' + str(steps_values[i])\n",
    "  snap_path = base_path_s + 'bs' + str(batch_values[i]) + '_st' + str(steps_values[i])\n",
    "  tens_path = base_path_t + 'bs' + str(batch_values[i]) + '_st' + str(steps_values[i])\n",
    "  \n",
    "  print(\"\\n\")\n",
    "  print(batch_size, steps)\n",
    "  arg_list = ['--epochs=15', steps, batch_size, '--backbone=resnet50', '--fl-gamma=1.5', '--fl-alpha=0.25', snap_path, '--random-transform', tens_path, 'csv', '/content/gdrive/My Drive/BA/Data/train_set_v2_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_v2_retina.csv']\n",
    "  \n",
    "  main(arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtpUbhYJakWW"
   },
   "outputs": [],
   "source": [
    "## Focal Loss grid search\n",
    "gamma_values = [0.5,    1,  1.5,  3.5,    5]\n",
    "alpha_values = [0.5, 0.25, 0.25, 0.25, 0.25]\n",
    "base_path_s = '--snapshot-path=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/FL_finetune/'\n",
    "base_path_t = '--tensorboard-dir=/content/gdrive/My Drive/BA/RetinaNet/Model/resnet50/FL_finetune/'\n",
    "\n",
    "\n",
    "for i in range(len(gamma_values)):\n",
    "  \n",
    "  fl_gamma = '--fl-gamma=' + str(gamma_values[i])\n",
    "  fl_alpha = '--fl-alpha=' + str(alpha_values[i])\n",
    "  snap_path = base_path_s + 'g' + str(gamma_values[i]) + '_a' + str(alpha_values[i])\n",
    "  tens_path = base_path_t + 'g' + str(gamma_values[i]) + '_a' + str(alpha_values[i])\n",
    "  \n",
    "  print(\"\\n\")\n",
    "  print(fl_gamma, fl_alpha)\n",
    "  arg_list = ['--epochs=4', '--steps=1000', '--backbone=resnet50', fl_gamma, fl_alpha, snap_path, '--random-transform', tens_path, 'csv', '/content/gdrive/My Drive/BA/Data/train_set_12_pw_retina.csv', '/content/gdrive/My Drive/BA/Data/classes.csv', '--val-annotations=/content/gdrive/My Drive/BA/Data/test_set_12_pw_retina.csv']\n",
    "  \n",
    "  main(arg_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UMA1_eZKxb0_",
    "Y1g_gkgoNR8a"
   ],
   "name": "RetinaNet_self_train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (tal)",
   "language": "python",
   "name": "tal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
